# Movies ETL Project (PySpark + PostgreSQL) ‚Äî End‚Äëto‚ÄëEnd

This repository contains a **complete ETL pipeline** for a (very large) movies ratings dataset packaged as `movies.zip`. It also ships with a **small sample** so you can try everything quickly. The dataset link is given below:

https://www.kaggle.com/datasets/chaitanyahivlekar/large-movie-dataset

```
ETL/
‚îú‚îÄ .vscode/
‚îÇ  ‚îî‚îÄ launch.json
‚îú‚îÄ extract/
‚îÇ  ‚îî‚îÄ execute.py
‚îú‚îÄ transform/
‚îÇ  ‚îî‚îÄ execute.py
‚îú‚îÄ load/
‚îÇ  ‚îî‚îÄ execute.py
‚îú‚îÄ utility/
‚îÇ  ‚îî‚îÄ utility.py
‚îú‚îÄ config/
‚îÇ  ‚îî‚îÄ config.yaml
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/        # extracted files from movies.zip go here
‚îÇ  ‚îú‚îÄ processed/  # outputs written by transform step
‚îÇ  ‚îî‚îÄ sample/     # tiny CSV for quick testing
‚îú‚îÄ app/
‚îÇ  ‚îî‚îÄ streamlit_app.py
‚îú‚îÄ scripts/
‚îÇ  ‚îú‚îÄ docker-compose.yml
‚îÇ  ‚îî‚îÄ init_db.sql
‚îú‚îÄ execute_all.py
‚îú‚îÄ extract.log
‚îú‚îÄ transform.log
‚îî‚îÄ load.log
```

## üß© What does each step do?

- **extract/execute.py**
  - Unzips `movies.zip` into `data/raw/`
  - Optionally creates a small sample CSV for testing (default 20k rows).

- **transform/execute.py** (PySpark)
  - Reads the CSV (sample or full).
  - Cleans columns, extracts `year` from title `(YYYY)`, splits `genre` into multiple rows.
  - Writes:
    - `movies_clean.parquet` (clean dataset for reuse)
    - Aggregations as CSV (for Postgres):
      - `genre_stats_csv/`
      - `year_stats_csv/`
      - `top_movies_csv/`

- **load/execute.py** (PostgreSQL)
  - Creates tables if missing.
  - Bulk loads the CSVs using `COPY`.
  - Tables created: `genre_stats`, `year_stats`, `top_movies`.

- **app/streamlit_app.py** (Visualization)
  - A simple web app to browse the results from Postgres:
    - Bar chart of ratings by **genre**
    - Line chart of ratings by **year**
    - Table of **Top Movies** by avg rating (with minimum review slider)

---

## üöÄ Quick Start (Non‚Äëtechnical)

> These steps are designed for Windows, macOS, or Linux. No prior experience required.

### 1) Install Python

- Download **Python 3.10+** from https://www.python.org/downloads/  
- During installation, check **"Add Python to PATH"**.

### 2) Install Java (required by PySpark)

- Install **Temurin OpenJDK 11**:
  - Windows/macOS: https://adoptium.net/temurin/releases/
  - Linux (Ubuntu): `sudo apt-get install -y openjdk-11-jdk`
- After installing, restart your terminal.

### 3) Install Docker (to run PostgreSQL easily)

- Install **Docker Desktop**: https://www.docker.com/get-started/
- Start Docker Desktop.

> If you cannot use Docker, you can install PostgreSQL manually and update the DB settings in `config/config.yaml`.

### 4) Download this project

- If you got this as a `movies_etl_project.zip`, **unzip** it to a convenient folder (e.g., Desktop).  
- Put your `movies.zip` (the big dataset file you have) **in the project root**, next to `execute_all.py`.

### 5) Open a terminal in the project folder and run:

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate

pip install -r requirements.txt
```

### 6) Start PostgreSQL with Docker

```bash
cd scripts
docker compose up -d
cd ..
```

This starts Postgres at `localhost:5432` with:
- database: `movies`
- user: `etl_user`
- password: `etl_pass`

### 7) Run the full ETL

> Make sure `movies.zip` is in the project root.  
> By default we use a **small sample** to keep things fast. You can switch to full dataset later.

```bash
python execute_all.py
```

You will see `extract.log`, `transform.log`, `load.log` being updated.  
On first run, it may take a few minutes depending on your computer.

### 8) Launch the Dashboard

```bash
streamlit run app/streamlit_app.py
```
The browser will open automatically with the dashboard.

---

## üì¶ Switching to the Full Dataset

- Open `config/config.yaml` and set:
  - `transform.use_sample_only: false`
- Run again:
  ```bash
  python execute_all.py
  ```

> The full CSV is **huge (‚âà1.6 GB)** and will take longer. Make sure you have enough disk space and patience.

---

## ‚öôÔ∏è Configuration

Edit `config/config.yaml`:

- `paths.zip_path`: Where `movies.zip` is. If you place it in the project root, keep it as `"./movies.zip"`.
- `extract.make_sample`: `true/false` to create the small sample automatically.
- `transform.use_sample_only`: `true` (fast) or `false` (full data).
- `db`: Database connection details. Can be overridden by environment variables `DB_HOST`, `DB_PORT`, `DB_NAME`, `DB_USER`, `DB_PASS`.

---

## üß™ VS Code Debug Buttons

Open this folder in **VS Code**. In the **Run and Debug** panel you will see:

- **ETL: Extraction**
- **ETL: Transform**
- **ETL: Load**

These run the three steps individually with the config file.

---

## üß∞ Requirements

Install them with `pip install -r requirements.txt`:

- `pyspark`
- `psycopg2-binary`
- `pandas`
- `pyyaml`
- `streamlit`

---

